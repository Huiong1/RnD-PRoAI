<h1> í¬ë¡¤ë§ì— ëŒ€í•˜ì—¬ </h1>
(ì£¼)ìŠˆí¼íˆì–´ë¡œë¡œë¶€í„° ë°ì´í„°ë“¤ì„ ë°›ì„ ì˜ˆì •.<br>
ì–´ë– í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆì„ì§€ ëª¨ë¦„. (hwp, ppt ë“± ê°€ì§€ê°ìƒ‰ì¼ê²ƒìœ¼ë¡œ ì˜ˆìƒ.) <br>
ê¸°ì—…ì—ì„œ 1ì°¨ì ìœ¼ë¡œ ì •ì œí•´ì„œ ë°ì´í„°ë¥¼ ë³´ë‚´ì¤€ë‹¤ê³  í•˜ì˜€ìŒ.

<h2>í¬ë¡¤ë§ ê¸°ë²•</h2>
í¬ë¡¤ë§ì€ ì›¹ í˜ì´ì§€ì—ì„œ íŠ¹ì • ë°ì´í„°ë“¤ì„ ì¶”ì¶œí•˜ëŠ” ì‘ì—…ì´ë‹¤.</br>
ìŠ¤í¬ë˜í•‘ê³¼ ë¹„ìŠ·í•œ ê°œë…ì´ì§€ë§Œ ë‹¤ë¥´ë‹¤. ( ìŠ¤í¬ë˜í•‘ì€ ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ëŒì–´ì˜¤ëŠ” ê²ƒ. í¬ë¡¤ë§ì€ ë¶„ë¥˜ ë° ìƒ‰ì¸ ì‘ì—…ê¹Œì§€ ì™„ë£Œí•˜ëŠ”ê²ƒ.)<br>
<h2>í¬ë¡¤ë§ì˜ ì „ê°œê³¼ì •</h2>
<ol>
    <li>ë°ì´í„°ë¥¼ ì¶”ì¶œí•  urlì„ ë³€ìˆ˜ì— ì €ì¥í•œë‹¤.
    <li>ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.
    <li>ì›¹ì‚¬ì´íŠ¸ ì½”ë“œì—ì„œ íŠ¹ì • íƒœê·¸ë¥¼ ì§€ì •í•œë‹¤.
    <li>í•„ìš”ì‹œ íŠ¹ì •íƒœê·¸ì—ì„œ íŠ¹ì • í´ë˜ìŠ¤ê¹Œì§€ ì§€ì •ê°€ëŠ¥í•˜ë‹¤.
    <li>íŠ¹ì • íƒœê·¸ì™€ í´ë˜ìŠ¤ì— ë‹´ê²¨ì ¸ ìˆëŠ” ë°ì´í„°ë“¤ì„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•œë‹¤.
    <li>í•´ë‹¹ ë¦¬ìŠ¤íŠ¸ ì •ë³´ë¥¼ xlsxì´ë‚˜ csvë¡œ ì €ì¥í•œë‹¤.
</ol>
<h2>ì‚¬ìš©ì˜ˆì‹œ</h2>

```python
	base_url = "urlëª…"
	ë°ì´í„°ë¥¼ ëŒì–´ì˜¬ urlì˜ ì •ë³´ë¥¼ ë³€ìˆ˜ì— ë‹´ëŠ”ë‹¤. ì´ ì½”ë“œì—ì„œ urlëª…ì€ ê¸°ì‚¬ ê²€ìƒ‰ ê²°ê³¼ urlì‚¬ìš©.
	list_url = []  # ê¸°ì‚¬ URLì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
	ê²€ìƒ‰ê²°ê³¼ë¡œ ë‚˜ì˜¨ ê¸°ì‚¬ë“¤ì˜ urlë“¤ì„ ì €ì¥í•´ì•¼í•˜ê¸°ì— ì‚¬ìš©.

	for i in range(0, 5):  # 0ë¶€í„° 370ê¹Œì§€ ë°˜ë³µ
    		url = base_url + str(i * 10 + 1)  # ë„¤ì´ë²„ ë‰´ìŠ¤ í˜ì´ì§€ ë²ˆí˜¸
    		driver.get(url)
    		time.sleep(1)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

   	 	# í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì™€ì„œ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
    		html = driver.page_source
    		soup = BeautifulSoup(html, 'html.parser')

		# ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ ì°¾ê¸° (ë„¤ì´ë²„ ë‰´ìŠ¤ì˜ ê²½ìš° `.news_tit` ì‚¬ìš©)
    		for link in soup.find_all("a", class_="news_tit"):
        		if link.has_attr("href"):  # href ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
            			news_url = link["href"] # href ì†ì„±ì´ ìˆìœ¼ë©´ news_urlì— ì €ì¥ì¥
            			print("âœ… Found URL:", news_url) #news_url ì¶œë ¥ë ¥
            			list_url.append(news_url) #list_urlì— news_url append
        

	# ë¸Œë¼ìš°ì € ì¢…ë£Œ
	driver.quit()

	# ë°ì´í„°ë¥¼ crawled_data.csvì— ì €ì¥ (ì¤‘ê°„ì— í¬ë¡¤ë§ì´ ë©ˆì¶°ë„ ì €ì¥ë˜ë„ë¡ í•¨)
	data_url = pd.DataFrame(list_url, columns=['url'])
	data_url.to_csv('crawled_data.csv', encoding='cp949', index=False)

	print("âœ… í¬ë¡¤ë§ ì™„ë£Œ! ë‰´ìŠ¤ ë§í¬ê°€ crawled_data.csvì— ì €ì¥ë¨!")
	print("íŒŒì¼ ì €ì¥ ìœ„ì¹˜:", os.path.abspath("crawled_data.csv"))
	# âœ… CSV íŒŒì¼ì—ì„œ URL ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (ê²½ë¡œ ìˆ˜ì •)
	df = pd.read_csv("./crawled_data.csv")
	urls = df["url"]

	# ChromeDriver ê²½ë¡œ
	CHROMEDRIVER_PATH = "C:/dev_python/Webdriver/chromedriver.exe"

	# Chrome ì˜µì…˜ ì„¤ì •
	chrome_options = Options()
	chrome_options.add_argument("--start-maximized")

	# âœ… í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰ (í•œ ë²ˆë§Œ ì‹¤í–‰)
	service = Service(CHROMEDRIVER_PATH)
	driver = webdriver.Chrome(service=service, options=chrome_options)
	driver.implicitly_wait(5)  # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°

	content_list = []

	#íŒŒì´ì¬ dictionary
	site_more_button_selectors = {
	    "news.naver.com": "a.u_cbox_btn_more",  # ë„¤ì´ë²„
	    "daum.net": "button.link_fold",  # ë‹¤ìŒ (ì˜ˆì œ)
	    "news.sbs.co.kr": "button.more_btn",  # SBS ë‰´ìŠ¤ (ì˜ˆì œ)
	    "yna.co.kr" : "button.btn-type300.style20.arr01" #ì—°í•©ë‰´ìŠ¤
	}

	site_comment_button_selectors = {
	    "naver.com": "a.pi_btn_count",  # ë„¤ì´ë²„
	    "daum.net": "button.open_comments",  # ë‹¤ìŒ (ì˜ˆì œ)
	    "yna.co.kr" : "button.btn-type300.style20.arr01" #ì—°í•©ë‰´ìŠ¤
	}


	# âœ… ê° ë‰´ìŠ¤ ê¸°ì‚¬ URL ë°©ë¬¸ & ëŒ“ê¸€ í¬ë¡¤ë§
	for i, url in enumerate(urls[:1326]):  # ìµœëŒ€ 1326ê°œ ê¸°ì‚¬ í¬ë¡¤ë§
	    print(f"ğŸ” ({i+1}/{len(urls)}) URL í¬ë¡¤ë§ ì¤‘: {url}")
	    driver.get(url)
	    time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

	    site_domain = None
	    for domain in site_more_button_selectors.keys():
	        if domain in url:
	            site_domain = domain
	            break

	    if not site_domain:
	        print("âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸! ê³„ì† ì§„í–‰")
	        continue

	    # âœ… ëŒ“ê¸€ì°½ ì—´ê¸° ë²„íŠ¼ í´ë¦­ (ì˜ˆì™¸ ì²˜ë¦¬)
	    try:
	        comment_selector = site_comment_button_selectors.get(site_domain)
	        if comment_selector:
	            comment_btn = WebDriverWait(driver, 5).until(
	                EC.element_to_be_clickable((By.CSS_SELECTOR, comment_selector))
	            )
     	        comment_btn.click()
   	        time.sleep(2)
 	 except:
 	       print("âš ï¸ ëŒ“ê¸€ ë²„íŠ¼ ì—†ìŒ! ê³„ì† ì§„í–‰")
        
    #í•´ë‹¹ ë„ë©”ì¸ì— ì €ì¥ë¼ìˆëŠ” íƒœê·¸ë¥¼ ì°¾ì•„ì„œ ì €ì¥ì‹œì¼œë¼ë¼

    # âœ… 'ë”ë³´ê¸°' ë²„íŠ¼ ë°˜ë³µ í´ë¦­ (ì‚¬ì´íŠ¸ë³„ ë‹¤ë¥¸ ì„ íƒì ì‚¬ìš©)
    more_selector = site_more_button_selectors.get(site_domain)

    while True:
        try:
            more_btn = WebDriverWait(driver, 2).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, more_selector))
            )
            more_btn.click()
            time.sleep(1.5)
        except:
            break  # ë” ì´ìƒ ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ

    # âœ… ëŒ“ê¸€ ë‚´ìš© í¬ë¡¤ë§
    comments = driver.find_elements(By.CSS_SELECTOR, 'span[style="white-space: pre-line"]')
    for comment in comments:
        content_list.append(comment.text)

# í¬ë¡¤ë§ ì¢…ë£Œ
driver.quit()

# âœ… ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥
df = pd.DataFrame({'comment': content_list})

df.to_csv('2017_0809-1231.csv', encoding='utf-8-sig', index=False)

print("âœ… í¬ë¡¤ë§ ì™„ë£Œ! ë°ì´í„°ê°€ 2017_0809-1231.csvì— ì €ì¥ë¨.")
```

