from selenium import webdriver
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import time
import pandas as pd
import os
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰ (ê²½ë¡œëŠ” ì•Œì•„ì„œ ë³€ê²½! - ì˜ˆì‹œë¡œ ë‘ì—ˆìŒ)
driver = webdriver.Chrome()

# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ URL (ìœ¤ì„ì—´ íƒ„í•µ ê´€ë ¨ ê²€ìƒ‰)
base_url = "https://search.naver.com/search.naver?sm=tab_hty.top&where=news&ssc=tab.news.all&query=%EA%B9%80%EC%88%98%ED%98%84+%EC%97%B0%ED%95%A9%EB%89%B4%EC%8A%A4&oquery=%EC%97%B0%ED%95%A9%EB%89%B4%EC%8A%A4+%EC%A2%8C%ED%8C%8C&tqi=i90U%2Fdqo1LVss51OK7lssssstuR-054504"

list_url = []  # ê¸°ì‚¬ URLì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

for i in range(0, 5):  # 0ë¶€í„° 370ê¹Œì§€ ë°˜ë³µ
    url = base_url + str(i * 10 + 1)  # ë„¤ì´ë²„ ë‰´ìŠ¤ í˜ì´ì§€ ë²ˆí˜¸
    driver.get(url)
    time.sleep(1)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

    # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì™€ì„œ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    # ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ ì°¾ê¸° (ë„¤ì´ë²„ ë‰´ìŠ¤ì˜ ê²½ìš° `.news_tit` ì‚¬ìš©)
    for link in soup.find_all("a", class_="news_tit"):
        if link.has_attr("href"):  # href ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
            news_url = link["href"] # href ì†ì„±ì´ ìˆìœ¼ë©´ news_urlì— ì €ì¥ì¥
            print("âœ… Found URL:", news_url) #news_url ì¶œë ¥ë ¥
            list_url.append(news_url) #list_urlì— news_url append
        

# ë¸Œë¼ìš°ì € ì¢…ë£Œ
driver.quit()

# ë°ì´í„°ë¥¼ crawled_data.csvì— ì €ì¥ (ì¤‘ê°„ì— í¬ë¡¤ë§ì´ ë©ˆì¶°ë„ ì €ì¥ë˜ë„ë¡ í•¨)
data_url = pd.DataFrame(list_url, columns=['url'])
data_url.to_csv('crawled_data.csv', encoding='cp949', index=False)

print("âœ… í¬ë¡¤ë§ ì™„ë£Œ! ë‰´ìŠ¤ ë§í¬ê°€ crawled_data.csvì— ì €ì¥ë¨!")
print("íŒŒì¼ ì €ì¥ ìœ„ì¹˜:", os.path.abspath("crawled_data.csv"))


# âœ… CSV íŒŒì¼ì—ì„œ URL ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (ê²½ë¡œ ìˆ˜ì •)
df = pd.read_csv("./crawled_data.csv")
urls = df["url"]

# ChromeDriver ê²½ë¡œ
CHROMEDRIVER_PATH = "C:/dev_python/Webdriver/chromedriver.exe"

# Chrome ì˜µì…˜ ì„¤ì •
chrome_options = Options()
chrome_options.add_argument("--start-maximized")

# âœ… í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰ (í•œ ë²ˆë§Œ ì‹¤í–‰)
service = Service(CHROMEDRIVER_PATH)
driver = webdriver.Chrome(service=service, options=chrome_options)
driver.implicitly_wait(5)  # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°

content_list = []

#íŒŒì´ì¬ dictionary
site_more_button_selectors = {
    "news.naver.com": "a.u_cbox_btn_more",  # ë„¤ì´ë²„
    "daum.net": "button.link_fold",  # ë‹¤ìŒ (ì˜ˆì œ)
    "news.sbs.co.kr": "button.more_btn",  # SBS ë‰´ìŠ¤ (ì˜ˆì œ)
    "yna.co.kr" : "button.btn-type300.style20.arr01" #ì—°í•©ë‰´ìŠ¤
}

site_comment_button_selectors = {
    "naver.com": "a.pi_btn_count",  # ë„¤ì´ë²„
    "daum.net": "button.open_comments",  # ë‹¤ìŒ (ì˜ˆì œ)
    "yna.co.kr" : "button.btn-type300.style20.arr01" #ì—°í•©ë‰´ìŠ¤
}


# âœ… ê° ë‰´ìŠ¤ ê¸°ì‚¬ URL ë°©ë¬¸ & ëŒ“ê¸€ í¬ë¡¤ë§
for i, url in enumerate(urls[:1]):  # ìµœëŒ€ 1326ê°œ ê¸°ì‚¬ í¬ë¡¤ë§
    print(f"ğŸ” ({i+1}/{len(urls)}) URL í¬ë¡¤ë§ ì¤‘: {url}")
    driver.get(url)
    time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

    site_domain = None
    for domain in site_more_button_selectors.keys():
        if domain in url:
            site_domain = domain
            break

    if not site_domain:
        print("âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸! ê³„ì† ì§„í–‰")
        continue

    # âœ… ëŒ“ê¸€ì°½ ì—´ê¸° ë²„íŠ¼ í´ë¦­ (ì˜ˆì™¸ ì²˜ë¦¬)
    try:
        comment_selector = site_comment_button_selectors.get(site_domain)
        if comment_selector:
            comment_btn = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, comment_selector))
            )
            comment_btn.click()
            time.sleep(2)
    except:
        print("âš ï¸ ëŒ“ê¸€ ë²„íŠ¼ ì—†ìŒ! ê³„ì† ì§„í–‰")
        
    #í•´ë‹¹ ë„ë©”ì¸ì— ì €ì¥ë¼ìˆëŠ” íƒœê·¸ë¥¼ ì°¾ì•„ì„œ ì €ì¥ì‹œì¼œë¼ë¼

    # âœ… 'ë”ë³´ê¸°' ë²„íŠ¼ ë°˜ë³µ í´ë¦­ (ì‚¬ì´íŠ¸ë³„ ë‹¤ë¥¸ ì„ íƒì ì‚¬ìš©)
    more_selector = site_more_button_selectors.get(site_domain)

    while True:
        try:
            more_btn = WebDriverWait(driver, 2).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, more_selector))
            )
            more_btn.click()
            time.sleep(1.5)
        except:
            break  # ë” ì´ìƒ ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ

    # âœ… ëŒ“ê¸€ ë‚´ìš© í¬ë¡¤ë§
    comments = driver.find_elements(By.CSS_SELECTOR, 'span[style="white-space: pre-line"]')
    for comment in comments:
        content_list.append(comment.text)


# âœ… í¬ë¡¤ë§ ì¢…ë£Œ
driver.quit()

# âœ… ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥
df = pd.DataFrame({'comment': content_list})

df.to_csv('2017_0809-1231.csv', encoding='utf-8-sig', index=False)

print("âœ… í¬ë¡¤ë§ ì™„ë£Œ! ë°ì´í„°ê°€ 2017_0809-1231.csvì— ì €ì¥ë¨.")

